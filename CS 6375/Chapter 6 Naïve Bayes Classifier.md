# Naïve Bayes Classifier
- The Bayesian approach to classifying a new instance is to assign the most probable target value, $V_{map}$, given the attribute values that describe the instance. 
- $V_{map}= argmax_{v_{j} \in V}P(v_{j}|a_{1}, \dots, a_{n})$. 
- We can use Bayes theorem to rewrite this expression as $V_{map}=argmax_{v_{j} \in V}P(a_{1}, \dots, a_{n}|v_{j})P(v_{j})$. 
- We could attempt to estimate the two terms used for calculating the $V_{map}$ based on the training data. It is easy to estimate each of the $P(v_{j})$ simply by counting the frequency with which each target value $v_{j}$ occurs in the training data. 
- The naïve Bayes classifier is based on the simplifying assumption that the attribute values are conditionally independent given the target value. In other words, the assumption is that given the target value of the instance, the probability of observing the conjunction $a_{1}, \dots, a_{n}$ is just the product of the probabilities of the individual attributes: $P(a_{1}, \dots, a_{n}|v_{j})=\Pi_{i}P(a_{i}|v_{j})$. 
- Naïve Bayes Classifier: $v_{NB}=argmax_{v_{j} \in V}P(v_{j})\Pi_{i}P(a_{i}|v_{j})$. 
- Whenever the naïve Bayes assumption of conditional independence is satisfied, this naïve Bayes classification is identical to the MAP classification. 
- More generally, when $X$ contains $n$ attributes which satisfy the conditional independence assumption, we have $P(X_{1}, \dots, X_{n}|Y)=\Pi^{n}_{i=1}p(X_{i}|Y)$. Notice that when $Y$ and $X_{i}$ are boolean variables, we need only $2n$ parameters to define $P(X_{i}=x_{ik}|Y=y_{j})$. We calculate this make $d$ passes through our data $D$. Making our model fitting algorithm $O(nd)$. 
- When $n$ attributes $X_{i}$ each take on $J$ possible discrete values, and $Y$ is a discrete variable taking on $K$ possible values, then our learning task is to estimate two sets of parameters $\boldsymbol{\theta_{ijk}}\equiv P(X_{i}=x_{ij}|Y=y_{k})$ for each input attribute $X_{i}$, each of its possible values $x_{ij}$, and each of the possible values $y_{k}$ of $Y$. Note that there will be $nJK$ such parameters.
- Maximum likelihood estimates for $\boldsymbol{\theta_{ijk}}$ given a set of training examples $\mathcal{D}$ are given by $\hat{\boldsymbol{\theta_{ijk}}}= \frac{D\{X_{i}=x_{ij} \cap Y=y_{k}\}}{ D\{Y=y_{k}\}}$ where the $D\{x\}$ operator returns the  number of elements in the set of $D$ that satisfy property $x$. 
- One danger of this maximum likelihood estimate is that it can sometimes result in the estimate of $\boldsymbol{\hat{ \theta}}_{ijk}$ being zero, if the data does not happen to contain any training examples satisfying the condition in the numerator. To avoid this, it is common to use a "smoothed" estimate which effectively adds in a number of additional hallucinated examples, and which assumes these hallucinated examples are spread evenly over the possible values of $X_{i}$. 
- This smoothed estimate is give by $\hat{ \boldsymbol{\theta}}_{ijk}=\frac{D\{X_{i}=x_{ij}\cap Y=y_{k}\}+l}{D\{Y=y_{k}\}+lJ}$ where $J$ is the number of distinct values $X_{i}$ can take on, and $l$ determines the strength of this smoothing. 
- If $l$ is set to 1, this approach is called Laplace smoothing.
- 