# Naïve Bayes Classifier
- The Bayesian approach to classifying a new instance is to assign the most probable target value, $V_{map}$, given the attribute values that describe the instance. 
- $V_{map}= argmax_{v_{j} \in V}P(v_{j}|a_{1}, \dots, a_{n})$. 
- We can use Bayes theorem to rewrite this expression as $V_{map}=argmax_{v_{j} \in V}P(a_{1}, \dots, a_{n}|v_{j})P(v_{j})$. 
- We could attempt to estimate the two terms used for calculating the $V_{map}$ based on the training data. It is easy to estimate each of the $P(v_{j})$ simply by counting the frequency with which each target value $v_{j}$ occurs in the training data. 
- The naïve Bayes classifier is based on the simplifying assumption that the attribute values are conditionally independent given the target value. In other words, the assumption is that given the target value of the instance, the probability of observing the conjunction $a_{1}, \dots, a_{n}$ is just the product of the probabilities of the individual attributes: $P(a_{1}, \dots, a_{n}|v_{j})=\Pi_{i}P(a_{i}|v_{j})$. 
- Naïve Bayes Classifier: $v_{NB}=argmax_{v_{j} \in V}P(v_{j})\Pi_{i}P(a_{i}|v_{j})$. 
- Whenever the naïve Bayes assumption of conditional independence is satisfied, this naïve Bayes classification is identical to the MAP classification. 
- More generally, when $X$ contains $n$ attributes which satisfy the conditional independence assumption, we have $P(X_{1}, \dots, X_{n}|Y)=\Pi^{n}_{i=1}p(X_{i}|Y)$. Notice that when $Y$ and $X_{i}$ are boolean variables, we need only $2n$ parameters to define $P(X_{i}=x_{ik}|Y=y_{j})$. We calculate this make $d$ passes through our data $D$. Making our model fitting algorithm $O(nd)$. 
- When $n$ attributes $X_{i}$ each take on $J$ possible discrete values, and $Y$ is a discrete variable taking on $K$ possible values, then our learning task is to estimate two sets of parameters $\boldsymbol{\theta_{ijk}}\equiv P(X_{i}=x_{ij}|Y=y_{k})$ for each input attribute $X_{i}$, each of its possible values $x_{ij}$, and each of the possible values $y_{k}$ of $Y$. Note that there will be $nJK$ such parameters.
- Maximum likelihood estimates for $\boldsymbol{\theta_{ijk}}$ given a set of training examples $\mathcal{D}$ are given by $\hat{\boldsymbol{\theta_{ijk}}}= \frac{D\{X_{i}=x_{ij} \cap Y=y_{k}\}}{ D\{Y=y_{k}\}}$ where the $D\{x\}$ operator returns the  number of elements in the set of $D$ that satisfy property $x$. 
- One danger of this maximum likelihood estimate is that it can sometimes result in the estimate of $\boldsymbol{\hat{ \theta}}_{ijk}$ being zero, if the data does not happen to contain any training examples satisfying the condition in the numerator. To avoid this, it is common to use a "smoothed" estimate which effectively adds in a number of additional hallucinated examples, and which assumes these hallucinated examples are spread evenly over the possible values of $X_{i}$. 
- This smoothed estimate is give by $\hat{ \boldsymbol{\theta}}_{ijk}=\frac{D\{X_{i}=x_{ij}\cap Y=y_{k}\}+l}{D\{Y=y_{k}\}+lJ}$ where $J$ is the number of distinct values $X_{i}$ can take on, and $l$ determines the strength of this smoothing. 
- If $l$ is set to 1, this approach is called Laplace smoothing.
## Naïve Bayes for Continuous Inputs
- In the case of continuous inputs $X_{i}$, we must choose some other way to represent the distributions $P(X_{i}|Y)$. 
- One common approach is to assume that for each possible discrete value $y_{k}$ of $Y$, the distribution of each continuous $X_{i}$ is gaussian, and is defined by a mean and standard deviation specific to $X_{i}$ and $y_{k}$. 
- $\mu_{ik}=E[X_{i}|Y=y_{k}]$ and $\sigma^{2}_{ik}=E[(X_{i}-\mu_{ik})^{2}|Y=y_{k}]$ for each attribute $X_{i}$ and each possible value $y_{k}$ of $Y$. Note there are $2nK$ of these parameters. 
- The maximum likelihood estimator for $\mu_{ik}=\frac{1}{\sum_{j}\delta(Y^{j}=y_{k})}\sum_{j}X^{J}_{i}\delta(Y^{j}=y_{k})$ where the superscript $j$ refers to the $j$th training example, and where $\delta(Y=y_{k})$ is 1 if $Y=y_{k}$ and 0 otherwise. 
- The maximum likelihood estimator for $\sigma^{2}_{ik}$ is $\hat{\sigma}^{2}_{ik}=\frac{1}{\sum_{j}(Y^{j}=y_{k})-1}\sum_{j}(X^{j}_{i}-\hat{\mu}_{ik})^{2}\delta(Y^{j}=y_{k})$. This is actually known as the minimum variance unbiased estimator. 
## Logistic Regression
- Logistic Regression is an approach to learning functions of the form $f:X \to Y$, or $P(Y|X)$ in the case where $Y$ is discrete-valued, and $X<X_{1}, \dots,X_{n}>$ is any vector containing discrete or continuous variables. 
- Logistic regression assumes a parametric form for the distribution $P(Y|X)$, then directly estimates is parameters from the training data. 
- The parametric model assumed by Logistic Regression in the case where $Y$ is boolean is $P(Y=1|X)=\frac{1}{1+e^{\left( w_{0}+\sum^{n}_{i=1}w_{i}X_{i} \right)}}$ and $P(Y=0|X)= \frac{e^{\left( w_{0}+\sum^{n}_{i=1}w_{i}X_{i} \right)}}{1+e^{\left( w_{0}+\sum^{n}_{i=1}w_{i}X_{i} \right)}}$. Notice that the second equation follows directly from the first, as these two probabilities must equal $1$. 
- One highly convenient property of this form for $P(Y|X)$ is that it leads to a simple linear expression for classification. To classify any given $X$ we generally want to assign the value $y_{k}$ that maximizes $P(Y=y_{k}|X)$. Put another way, we assign the label $Y=0$ if the following condition holds $1< \frac{P(Y=0|X)}{P(Y=1|X)}$. Substituting from the two equations for $P(Y=0|X)$ and $P(Y=1|X)$, this becomes $1<e^{\left( w_{0}+\sum^{n}_{i=1}w_{i}X_{i} \right)}$. Taking the natural log of both sides we have a linear classification rule that assigns label $Y=0$ if $X$ satisfies $0<w_{0}+\sum^{n}_{i=1}w_{i}X_{i}$ and assigns $Y=1$ otherwise. 
- Interestingly, the parametric form of $P(Y|X)$ used by logistic regression is precisely the form implied by the assumptions of a Gaussian Naïve Bayes classifier. 
## Form of $P(Y|X)$ for Gaussian Naïve Bayes Classifier
- We consider our GNB to be based on the following modeling assumptions: 
	- $Y$ is boolean, governed by a Bernoulli distribution, with parameter $\pi=P(Y=1)$. 
	- $X=<X_{1}, \dots,X_{n}>$, where each $X_{i}$ is a continuous random variable. 
	- For each $X_{i}$, $P(X_{i}|Y=y_{k})$ is a gaussian distribution of the form $N(\mu_{ik}, \sigma_{i})$. 
	- For all $i$ and $j \neq i$, $X_{i}$ and $X_{j}$ are conditionally independent given $Y$. 
- In general, Bayes rules allows us to write: $P(Y=1|X)=\frac{P(Y=1)P(X|Y=1)}{P(Y=1)P(X|Y=1)+P(Y=0)P(X|Y=0)}$. 
- Dividing both the numerator and denominator by the numerator yields: $P(Y=1|X)=\frac{1}{1+\frac{P(Y=0)P(X|Y=0)}{P(Y=1)P(X|Y=1)}}$ or equivalently $P(Y=1|X)=\frac{1}{1+e^{(\ln( \frac{P(Y=0)P(X|Y=0)}{P(Y=1)P(X|Y=1)} ))}}$. 
- Because of our conditional independent assumption we can write this $P(Y=1|X)=\frac{1}{1+e^{\ln \frac{P(Y=0)}{P(Y=1)}+\sum_{i}\ln \frac{P(X_{i}|Y=0)}{P(X_{i}|Y=1)}}}=\frac{1}{1+e^{\ln \frac{1-\pi}{\pi}+\sum_{i}\ln \frac{P(X_{i}|Y=0)}{P(X_{i}|Y=1)}}}$. 
- Now consider the summation in the denominator. Given our assumption that $P(X_{i}|Y=y_{k})$ is Gaussian, we can expand this term as follows: $\sum_{i}\ln \frac{\frac{1}{\sqrt{ 2\pi \sigma^2 }}e^{\frac{-(X_{i}-\mu_{i0})^2}{2\sigma^2}}}{\frac{1}{\sqrt{ 2\pi \sigma^2 }}e^{\frac{-(X_{i}-\mu_{i1})^2}{2\sigma^2}}}$ $= \sum_{i}\ln e^{\frac{(X_{i}-\mu _{i 1})^{2} - (X_{i}-\mu_{i 0})^2}{2\sigma^{2}_{i}}}$ $= \sum_{i} \frac{(X_{i}-\mu_{i 1})^{2} - (X_{i}-\mu_{i 0})^2}{2\sigma^{2}_{i}}$ $=\sum_{i}\frac{(X_{i}^{2}-2X_{i}\mu_{i 1}+\mu^{2}_{i 1})-(X_{i}^{2}-2X_{i}\mu_{i 0}+\mu^{2}_{i 0})}{2\sigma^{2}_{i}}$ $=\sum_{i}\frac{2X_{i}(\mu_{i 0}-\mu_{i 1})+\mu^2_{i 1}-\mu^{2}_{i 0}}{2\sigma^{2}_{i}}$ $= \sum_{i}\frac{\mu_{i 0}-\mu_{i 1}}{\sigma^{2}_{i}}X_{i} + \frac{\mu^{2}_{i 1}-\mu^{2}_{i 0}}{2\sigma^{2}_{i}}$. 
- Notice that this expression is a linear weighted sum of the $X_{i}$'s. Substituting this expression into our earlier expression we have $P(Y=1|X)=\frac{1}{1+e^{\ln \frac{1-\pi}{\pi}+\sum_{i}\frac{\mu_{i 0}-\mu_{i 1}}{\sigma^{2}_{i}}X_{i} + \frac{\mu^{2}_{i 1}-\mu^{2}_{i 0}}{2\sigma^{2}_{i}}}}$ or equivalently, $P(Y=1|X)= \frac{1}{1+e^{w_{0}+\sum^{n}_{i=1}w_{i}X_{i}}}$ where the weights $w_{1}\dots w_{n}$ are given by $w_{i}=\frac{\mu_{i 0}-\mu_{i 1}}{\sigma^{2}_{i}}$ and where $w_{0}= \ln \frac{1-\pi}{\pi}+\sum_{i} \frac{\mu^{2}_{i 1}-\mu^{2}_{i 0}}{2\sigma^{2}_{i}}$. 
## Estimating Parameters for Logistic Regression
- In many cases, we may suspect the Gaussian Naïve Bayes assumptions are not perfectly satisfied. In this case we may wish to estimate the $w_{i}$ parameters directly from the data, rather than going through the intermediate step of estimating the GNB parameters which forces us to adopt is more stringent modeling assumptions. 
- We can maximize the conditional data likelihood, or the probability of the observed $Y$ values in the training data, conditioned on their corresponding $X$ values by choosing parameters $W$ that satisfy $W = argmax_{w}\Pi_{l}P(Y^{l}|X^{l}, W)$. 
- We can work with the log likelihood of the conditional data likelihood to get $W = argmax \sum_{l}\ln P(Y^{l}|X^{l}, W)$ which we can write as an actual function: $\sum_{l}Y^{l}P(Y^{l}=1|X^{l}, W)+(1-Y^{l})\ln P(Y^{l}=0|X^{l}, W)$. Note here we are utilizing the fact that $Y$ can take only values 0 or 1, so only one of the two terms in the expression will be non-zero for any given $Y^{l}$.
- 
